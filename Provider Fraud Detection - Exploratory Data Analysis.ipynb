{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "35f70994",
   "metadata": {},
   "source": [
    "## Provider Fraud Detection - Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0da8f26f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Using incubator modules: jdk.incubator.vector\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/08/15 20:23:04 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "25/08/15 20:23:05 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark UI: http://mac.lan:4041\n"
     ]
    }
   ],
   "source": [
    "# Cell 1: Imports + SparkSession (creates one only if not present)\n",
    "import os\n",
    "import builtins as py\n",
    "import math\n",
    "import pandas as pd\n",
    "\n",
    "from pyspark.sql import SparkSession, functions as F\n",
    "from pyspark.sql.functions import col, lit\n",
    "\n",
    "# plotting (small samples only)\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Create a lightweight SparkSession if not already available\n",
    "if 'spark' not in globals():\n",
    "    spark = (SparkSession.builder\n",
    "             .appName(\"EDA_ProviderFraud\")\n",
    "             .master(\"local[*]\")\n",
    "             .config(\"spark.driver.memory\", \"6g\")          # lower for laptops\n",
    "             .config(\"spark.sql.shuffle.partitions\", \"4\")  # small for local dev\n",
    "             .config(\"spark.network.timeout\", \"600s\")\n",
    "             .getOrCreate())\n",
    "print(\"Spark UI:\", spark.sparkContext.uiWebUrl if spark.sparkContext.uiWebUrl else \"n/a\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b9999360",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Schema (preview):\n",
      "root\n",
      " |-- npi: string (nullable = true)\n",
      " |-- total_services: double (nullable = true)\n",
      " |-- total_beneficiaries: double (nullable = true)\n",
      " |-- total_bene_day_services: double (nullable = true)\n",
      " |-- w_avg_submitted_charge: double (nullable = true)\n",
      " |-- w_avg_allowed: double (nullable = true)\n",
      " |-- w_avg_payment: double (nullable = true)\n",
      " |-- charge_allowed_ratio: double (nullable = true)\n",
      " |-- payment_allowed_ratio: double (nullable = true)\n",
      " |-- num_unique_procedures: long (nullable = true)\n",
      " |-- stddev_submitted_charge: double (nullable = true)\n",
      " |-- frac_drug_services: double (nullable = true)\n",
      " |-- frac_missing_zip: double (nullable = true)\n",
      " |-- services_per_bene: double (nullable = true)\n",
      " |-- bene_days_per_bene: double (nullable = true)\n",
      " |-- primary_taxonomy: string (nullable = true)\n",
      " |-- is_fraud: integer (nullable = true)\n",
      " |-- label_is_inferred: integer (nullable = true)\n",
      " |-- state_abbr: string (nullable = true)\n",
      "\n",
      "\n",
      "Preview rows:\n",
      "+----------+--------+\n",
      "|npi       |is_fraud|\n",
      "+----------+--------+\n",
      "|1912999087|1       |\n",
      "|1811930357|1       |\n",
      "|1821181215|1       |\n",
      "|1861836942|0       |\n",
      "|1356817431|0       |\n",
      "|1588820955|0       |\n",
      "|1477818664|0       |\n",
      "|1255762373|0       |\n",
      "|1982965414|0       |\n",
      "|1790863884|0       |\n",
      "|1053952002|0       |\n",
      "|1710030812|0       |\n",
      "|1245200773|0       |\n",
      "|1285819268|0       |\n",
      "|1528059300|0       |\n",
      "|1679795942|0       |\n",
      "|1629126537|0       |\n",
      "|1912450081|0       |\n",
      "|1457882821|0       |\n",
      "|1659525756|0       |\n",
      "+----------+--------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 7:=================================================>       (14 + 2) / 16]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Approx total rows: 950347\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 7:=====================================================>   (15 + 1) / 16]\r"
     ]
    }
   ],
   "source": [
    "# Cell 2 (robust): Load merged parquet and quick preview (handles different Spark versions)\n",
    "MERGED_PATH = \"curated/training/providers_merged_asof_2023-12-31_ever.parquet\"\n",
    "\n",
    "if not os.path.exists(MERGED_PATH):\n",
    "    raise FileNotFoundError(f\"Expected merged parquet at {MERGED_PATH} — ensure you ran the wrangling step or adjust the path.\")\n",
    "\n",
    "df = spark.read.parquet(MERGED_PATH)\n",
    "\n",
    "print(\"Schema (preview):\")\n",
    "df.printSchema()\n",
    "\n",
    "print(\"\\nPreview rows:\")\n",
    "df.select(\"npi\", \"is_fraud\").limit(20).show(truncate=False)\n",
    "\n",
    "# ---- robust approx total rows extraction ----\n",
    "def get_count_approx(rdd, timeout_ms=2000, allow_fallback_to_exact=True):\n",
    "    \"\"\"Return an int estimate for rdd.countApprox(timeout_ms).\n",
    "       Handles Spark versions that return either dict or int.\n",
    "       If result type unknown and allow_fallback_to_exact is True, falls back to rdd.count().\"\"\"\n",
    "    try:\n",
    "        res = rdd.countApprox(timeout_ms)\n",
    "    except Exception as e:\n",
    "        print(\"countApprox raised:\", str(e))\n",
    "        if allow_fallback_to_exact:\n",
    "            print(\"Falling back to exact count() — this may take a while.\")\n",
    "            return rdd.count()\n",
    "        return None\n",
    "\n",
    "    # handle dict-like results: {deadline: estimate} or {\"1\": estimate}\n",
    "    if isinstance(res, dict):\n",
    "        vals = list(res.values())\n",
    "        if vals:\n",
    "            try:\n",
    "                return int(vals[0])\n",
    "            except Exception:\n",
    "                pass\n",
    "    # handle integer-like result\n",
    "    try:\n",
    "        return int(res)\n",
    "    except Exception:\n",
    "        # last resort: optional exact count\n",
    "        if allow_fallback_to_exact:\n",
    "            print(\"countApprox returned unexpected type; doing exact count() as fallback.\")\n",
    "            return rdd.count()\n",
    "        return None\n",
    "\n",
    "# Use the robust function\n",
    "approx_total = get_count_approx(df.rdd, timeout_ms=2000, allow_fallback_to_exact=False)\n",
    "if approx_total is None:\n",
    "    # fallback to a tiny sampled estimate if countApprox failed and we don't want an exact count\n",
    "    print(\"countApprox unavailable; using tiny sample to estimate total rows (cheap but noisy).\")\n",
    "    SAMPLE_F = 0.001\n",
    "    sample_count = df.sample(SAMPLE_F, seed=42).limit(10_000).count()  # limit to keep small\n",
    "    approx_total = int(sample_count / SAMPLE_F) if SAMPLE_F > 0 and sample_count > 0 else sample_count\n",
    "\n",
    "print(\"\\nApprox total rows:\", approx_total)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "75c0ab41",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/08/15 20:27:43 WARN SparkStringUtils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+--------------------+-------------------------+-------------------+--------------------------+----------------+----------------------+------------------------+\n",
      "|is_fraud_nulls|total_services_nulls|total_beneficiaries_nulls|w_avg_allowed_nulls|charge_allowed_ratio_nulls|state_abbr_nulls|primary_taxonomy_nulls|frac_drug_services_nulls|\n",
      "+--------------+--------------------+-------------------------+-------------------+--------------------------+----------------+----------------------+------------------------+\n",
      "|0             |0                   |0                        |0                  |0                         |2880            |3126                  |0                       |\n",
      "+--------------+--------------------+-------------------------+-------------------+--------------------------+----------------+----------------------+------------------------+\n",
      "\n",
      "Approx distinct state_abbr: 62\n",
      "Approx distinct primary_taxonomy: 622\n",
      "Approx distinct is_fraud: 2\n"
     ]
    }
   ],
   "source": [
    "# Cell 3: Null counts and approximate distinct counts for key columns\n",
    "cols_to_check = [\n",
    "    \"is_fraud\", \"total_services\", \"total_beneficiaries\", \"w_avg_allowed\",\n",
    "    \"charge_allowed_ratio\", \"state_abbr\", \"primary_taxonomy\", \"frac_drug_services\"\n",
    "]\n",
    "cols_present = [c for c in cols_to_check if c in df.columns]\n",
    "\n",
    "# Null counts\n",
    "null_exprs = [F.sum(F.when(F.col(c).isNull(), 1).otherwise(0)).alias(c + \"_nulls\") for c in cols_present]\n",
    "if null_exprs:\n",
    "    df.select(*null_exprs).show(truncate=False)\n",
    "\n",
    "# Approx distinct counts for categorical-ish columns\n",
    "cat_cols = [c for c in [\"state_abbr\", \"primary_taxonomy\", \"is_fraud\"] if c in df.columns]\n",
    "for c in cat_cols:\n",
    "    approx_dist = df.select(F.approx_count_distinct(c).alias(\"approx_dist\")).collect()[0][\"approx_dist\"]\n",
    "    print(f\"Approx distinct {c}: {approx_dist}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab8c2ad8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4: Numeric summary and approximate quantiles (cheap)\n",
    "num_cols = [c for c in [\"total_services\",\"total_beneficiaries\",\"w_avg_allowed\",\"charge_allowed_ratio\",\"frac_drug_services\",\"services_per_bene\"] if c in df.columns]\n",
    "\n",
    "if num_cols:\n",
    "    # mean & std\n",
    "    agg_exprs = [F.mean(c).alias(c + \"_mean\") for c in num_cols] + [F.stddev(c).alias(c + \"_std\") for c in num_cols]\n",
    "    df.select(*agg_exprs).show(truncate=False)\n",
    "\n",
    "    # approximate quantiles\n",
    "    qs = [0.01,0.05,0.25,0.5,0.75,0.95,0.99]\n",
    "    quantiles = df.stat.approxQuantile(num_cols, qs, 0.01)\n",
    "    for c, qvals in zip(num_cols, quantiles):\n",
    "        print(f\"\\n{c} quantiles:\")\n",
    "        for q,v in zip(qs, qvals):\n",
    "            print(f\"  {int(q*100)}% -> {v}\")\n",
    "else:\n",
    "    print(\"No numeric columns found for summary.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "492654b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5: Top outliers (limit) — quick checks\n",
    "if \"charge_allowed_ratio\" in df.columns:\n",
    "    print(\"Top charge_allowed_ratio (top 20):\")\n",
    "    df.select(\"npi\",\"charge_allowed_ratio\",\"total_services\",\"total_beneficiaries\").orderBy(F.col(\"charge_allowed_ratio\").desc()).limit(20).show(truncate=False)\n",
    "\n",
    "# Providers with very high services but tiny beneficiaries (possible anomalies)\n",
    "if all(c in df.columns for c in [\"total_services\",\"total_beneficiaries\"]):\n",
    "    df.filter((col(\"total_beneficiaries\") < 5) & (col(\"total_services\") > 1000)) \\\n",
    "      .select(\"npi\",\"total_services\",\"total_beneficiaries\").limit(20).show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1daa65f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6: Aggregations by state and by primary_taxonomy\n",
    "if \"state_abbr\" in df.columns:\n",
    "    by_state = (df.groupBy(\"state_abbr\")\n",
    "                  .agg(\n",
    "                      F.count(\"*\").alias(\"providers\"),\n",
    "                      F.sum(\"total_services\").alias(\"sum_services\"),\n",
    "                      F.avg(\"charge_allowed_ratio\").alias(\"mean_charge_ratio\"),\n",
    "                      F.sum(\"is_fraud\").alias(\"n_fraud\")\n",
    "                  )\n",
    "                  .orderBy(F.col(\"providers\").desc()))\n",
    "    display_rows = 50\n",
    "    print(\"By-state summary (top):\")\n",
    "    by_state.show(display_rows, truncate=False)\n",
    "else:\n",
    "    print(\"No state_abbr column found.\")\n",
    "\n",
    "if \"primary_taxonomy\" in df.columns:\n",
    "    by_tax = (df.groupBy(\"primary_taxonomy\")\n",
    "                .agg(\n",
    "                    F.count(\"*\").alias(\"providers\"),\n",
    "                    F.avg(\"charge_allowed_ratio\").alias(\"mean_charge_ratio\"),\n",
    "                    F.sum(\"is_fraud\").alias(\"n_fraud\")\n",
    "                )\n",
    "                .orderBy(F.col(\"providers\").desc()))\n",
    "    print(\"\\nBy-primary_taxonomy summary (top 50):\")\n",
    "    by_tax.show(50, truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d043dbb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 7: Compare fraud vs non-fraud for several metrics (medians/means)\n",
    "metrics = [c for c in [\"charge_allowed_ratio\",\"w_avg_allowed\",\"total_services\",\"total_beneficiaries\",\"frac_drug_services\",\"services_per_bene\"] if c in df.columns]\n",
    "\n",
    "if metrics:\n",
    "    agg_exprs = []\n",
    "    for m in metrics:\n",
    "        agg_exprs.append(F.expr(f\"percentile_approx({m}, 0.5)\").alias(m + \"_median\"))\n",
    "        agg_exprs.append(F.mean(m).alias(m + \"_mean\"))\n",
    "    # per class\n",
    "    profile = df.groupBy(\"is_fraud\").agg(*agg_exprs)\n",
    "    print(\"Fraud vs non-fraud profile (median/mean):\")\n",
    "    profile.show(truncate=False)\n",
    "else:\n",
    "    print(\"No metrics available for fraud profiling.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ds-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
